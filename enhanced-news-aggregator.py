#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ–°é—»èšåˆå™¨ - é›†æˆ Tavily API
æ”¯æŒå¤šæºæœç´¢ï¼šTavily (æ·±åº¦) + Twitter (å®æ—¶)
"""

import json
import subprocess
import os
from datetime import datetime
from typing import List, Dict
from collections import defaultdict

class EnhancedNewsAggregator:
    def __init__(self):
        self.results = []
        self.seen_urls = set()
        self.tavily_available = bool(os.getenv('TAVILY_API_KEY'))
        
    def search_tavily(self, query: str, topic: str = "news", days: int = 3) -> List[Dict]:
        """ä½¿ç”¨Tavilyæœç´¢ï¼ˆAIä¼˜åŒ–çš„æœç´¢å¼•æ“ï¼‰"""
        if not self.tavily_available:
            print(f"  âš ï¸  Tavily APIæœªé…ç½®ï¼Œè·³è¿‡")
            return []
            
        try:
            cmd = [
                "node",
                "/Users/haoc/.openclaw/workspace/skills/tavily-search/scripts/search.mjs",
                query,
                "-n", "5",
                "--topic", topic
            ]
            
            if topic == "news":
                cmd.extend(["--days", str(days)])
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode != 0:
                print(f"  âš ï¸  Tavilyæœç´¢å¤±è´¥: {result.stderr[:100]}")
                return []
            
            # è§£æTavilyè¾“å‡º
            articles = []
            lines = result.stdout.split('\n')
            current_article = {}
            in_sources = False
            
            for line in lines:
                if line.strip() == "## Sources":
                    in_sources = True
                    continue
                    
                if not in_sources:
                    continue
                
                if line.startswith('- **') and '**' in line[4:]:
                    if current_article:
                        articles.append(current_article)
                    
                    # è§£ææ ‡é¢˜å’Œç›¸å…³æ€§
                    title_end = line.find('**', 4)
                    title = line[4:title_end]
                    
                    relevance = 100
                    if 'relevance:' in line:
                        try:
                            rel_str = line.split('relevance:')[1].split('%')[0].strip()
                            relevance = int(rel_str)
                        except:
                            pass
                    
                    current_article = {
                        'title': title,
                        'relevance_score': relevance / 100.0,
                        'source': 'tavily'
                    }
                    
                elif line.strip().startswith('http') and current_article:
                    current_article['url'] = line.strip()
                    
                elif line.strip() and not line.startswith('#') and current_article and 'text' not in current_article:
                    current_article['text'] = line.strip()
            
            if current_article:
                articles.append(current_article)
            
            return articles
            
        except Exception as e:
            print(f"  âš ï¸  Tavilyæœç´¢å¼‚å¸¸: {e}")
            return []
    
    def search_twitter(self, query: str, count: int = 10) -> List[Dict]:
        """ä½¿ç”¨birdæœç´¢Twitter"""
        try:
            cmd = ["bird", "search", query, "-n", str(count)]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode != 0:
                return []
            
            # è§£æbirdè¾“å‡º
            tweets = []
            lines = result.stdout.split('\n')
            current_tweet = {}
            
            for line in lines:
                if line.startswith('@'):
                    if current_tweet:
                        tweets.append(current_tweet)
                    parts = line.split('(')
                    if len(parts) >= 2:
                        current_tweet = {
                            'username': parts[0].strip(),
                            'display_name': parts[1].split(')')[0] if ')' in parts[1] else '',
                            'source': 'twitter'
                        }
                elif line.startswith('ğŸ“…'):
                    current_tweet['timestamp'] = line.replace('ğŸ“…', '').strip()
                elif line.startswith('ğŸ”—'):
                    current_tweet['url'] = line.replace('ğŸ”—', '').strip()
                elif line.strip() and not line.startswith('â”€'):
                    if 'text' not in current_tweet:
                        current_tweet['text'] = line.strip()
                    else:
                        current_tweet['text'] += ' ' + line.strip()
            
            if current_tweet:
                tweets.append(current_tweet)
                
            return tweets
            
        except Exception as e:
            print(f"  âš ï¸  Twitteræœç´¢å¤±è´¥: {e}")
            return []
    
    def calculate_relevance_score(self, item: Dict, keywords: List[str]) -> float:
        """è®¡ç®—ç›¸å…³æ€§å¾—åˆ†"""
        # Tavilyç»“æœå·²æœ‰ç›¸å…³æ€§åˆ†æ•°
        if item.get('source') == 'tavily' and 'relevance_score' in item:
            return item['relevance_score']
        
        # Twitterç»“æœéœ€è¦è®¡ç®—
        text = item.get('text', '').lower()
        score = 0.0
        
        for keyword in keywords:
            if keyword.lower() in text:
                score += 1.0
        
        if len(text) < 50:
            score *= 0.5
        
        if item.get('url'):
            score += 0.5
            
        return score
    
    def deduplicate(self, items: List[Dict]) -> List[Dict]:
        """å»é‡"""
        unique = []
        for item in items:
            url = item.get('url', '')
            if url and url not in self.seen_urls:
                self.seen_urls.add(url)
                unique.append(item)
        return unique
    
    def aggregate_by_category(self) -> Dict[str, List[Dict]]:
        """æŒ‰ç±»åˆ«èšåˆæœç´¢ - Tavilyä¼˜å…ˆï¼ŒTwitterè¡¥å……"""
        categories = {
            "AIçªç ´": {
                "tavily_query": "AI breakthrough DeepSeek Claude GPT-5 2026",
                "twitter_keywords": ["DeepSeek R1", "Claude Opus", "GPT-5", "Gemini 3"]
            },
            "ç§‘æŠ€å…¬å¸": {
                "tavily_query": "tech companies AI OpenAI Anthropic Google 2026",
                "twitter_keywords": ["OpenAI", "Anthropic", "Google AI", "Meta AI"]
            },
            "æ”¿ç­–ç»æµ": {
                "tavily_query": "Trump tariff policy China trade 2026",
                "twitter_keywords": ["Trump tariff", "ç‰¹æœ—æ™®å…³ç¨", "ä¸­ç¾è´¸æ˜“"]
            },
            "åŠ å¯†è´§å¸": {
                "tavily_query": "cryptocurrency Bitcoin Ethereum Web3 2026",
                "twitter_keywords": ["Bitcoin", "Ethereum", "crypto", "Web3"]
            }
        }
        
        results = defaultdict(list)
        
        for category, config in categories.items():
            print(f"\nğŸ” æœç´¢ç±»åˆ«: {category}")
            
            all_items = []
            
            # 1. Tavilyæœç´¢ï¼ˆæ·±åº¦å†…å®¹ï¼‰
            if self.tavily_available:
                print(f"  ğŸ“¡ Tavilyæœç´¢...")
                tavily_results = self.search_tavily(config['tavily_query'], topic="news", days=3)
                all_items.extend(tavily_results)
                print(f"    âœ“ Tavily: {len(tavily_results)} æ¡")
            
            # 2. Twitteræœç´¢ï¼ˆå®æ—¶åŠ¨æ€ï¼‰
            print(f"  ğŸ¦ Twitteræœç´¢...")
            twitter_query = " OR ".join([f'"{kw}"' for kw in config['twitter_keywords'][:3]])
            twitter_results = self.search_twitter(twitter_query, count=10)
            all_items.extend(twitter_results)
            print(f"    âœ“ Twitter: {len(twitter_results)} æ¡")
            
            # 3. è®¡ç®—ç›¸å…³æ€§å¹¶æ’åº
            scored_items = []
            for item in all_items:
                score = self.calculate_relevance_score(item, config['twitter_keywords'])
                if score > 0:
                    item['relevance_score'] = score
                    scored_items.append(item)
            
            scored_items.sort(key=lambda x: x['relevance_score'], reverse=True)
            unique_items = self.deduplicate(scored_items)
            
            # å–top 5
            results[category] = unique_items[:5]
            print(f"  âœ… æœ€ç»ˆ: {len(unique_items)} æ¡å»é‡ç»“æœ")
        
        return results
    
    def generate_briefing(self, categorized_results: Dict[str, List[Dict]]) -> str:
        """ç”Ÿæˆç®€æŠ¥"""
        now = datetime.now()
        briefing = f"# ğŸ“° æ¯æ—¥ç®€æŠ¥ {now.strftime('%Y-%m-%d')}\n\n"
        briefing += f"> ç”Ÿæˆæ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M')} GMT+8\n"
        briefing += f"> æ•°æ®æ¥æºï¼šTavily AI Search + X/Twitter\n"
        briefing += f"> æœç´¢ç­–ç•¥ï¼šæ·±åº¦å†…å®¹(Tavily) + å®æ—¶åŠ¨æ€(Twitter)\n\n"
        briefing += "---\n\n"
        
        category_icons = {
            "AIçªç ´": "ğŸ¤–",
            "ç§‘æŠ€å…¬å¸": "ğŸ¢",
            "æ”¿ç­–ç»æµ": "ğŸ“Š",
            "åŠ å¯†è´§å¸": "ğŸ’°"
        }
        
        for category, items in categorized_results.items():
            if not items:
                continue
                
            icon = category_icons.get(category, "ğŸ“Œ")
            briefing += f"## {icon} {category}\n\n"
            
            for i, item in enumerate(items, 1):
                source_icon = "ğŸ“¡" if item.get('source') == 'tavily' else "ğŸ¦"
                title = item.get('title', item.get('display_name', 'Unknown'))
                text = item.get('text', '')[:250]
                url = item.get('url', '')
                score = item.get('relevance_score', 0)
                
                briefing += f"### {i}. {source_icon} {title}\n"
                briefing += f"{text}\n\n"
                if url:
                    briefing += f"ğŸ”— {url}\n"
                briefing += f"ğŸ“Š ç›¸å…³æ€§: {score:.1f}\n\n"
            
            briefing += "---\n\n"
        
        briefing += "## ğŸ’¡ æœç´¢è´¨é‡æŠ¥å‘Š\n\n"
        total_results = sum(len(items) for items in categorized_results.values())
        tavily_count = sum(1 for items in categorized_results.values() for item in items if item.get('source') == 'tavily')
        twitter_count = sum(1 for items in categorized_results.values() for item in items if item.get('source') == 'twitter')
        
        briefing += f"- æ€»ç»“æœæ•°: {total_results}\n"
        briefing += f"- Tavilyæ·±åº¦å†…å®¹: {tavily_count}\n"
        briefing += f"- Twitterå®æ—¶åŠ¨æ€: {twitter_count}\n"
        briefing += f"- å»é‡åURL: {len(self.seen_urls)}\n"
        briefing += f"- è¦†ç›–ç±»åˆ«: {len([c for c, t in categorized_results.items() if t])}\n\n"
        
        briefing += "*æœ¬ç®€æŠ¥ä½¿ç”¨ Tavily AI Search + Twitter åŒæºèšåˆï¼Œå·²ä¼˜åŒ–æœç´¢è´¨é‡*\n"
        
        return briefing


def main():
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæ–°é—»èšåˆå™¨...")
    print(f"ğŸ“¡ Tavily API: {'âœ… å·²é…ç½®' if os.getenv('TAVILY_API_KEY') else 'âŒ æœªé…ç½®'}")
    
    aggregator = EnhancedNewsAggregator()
    
    # æŒ‰ç±»åˆ«èšåˆ
    results = aggregator.aggregate_by_category()
    
    # ç”Ÿæˆç®€æŠ¥
    briefing = aggregator.generate_briefing(results)
    
    # ä¿å­˜æ–‡ä»¶
    now = datetime.now()
    filename = f"/Users/haoc/.openclaw/workspace/briefing-{now.strftime('%Y-%m-%d')}-enhanced.md"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(briefing)
    
    print(f"\nâœ… ç®€æŠ¥å·²ç”Ÿæˆ: {filename}")
    print(f"ğŸ“Š æ€»è®¡ {sum(len(t) for t in results.values())} æ¡é«˜è´¨é‡ç»“æœ")


if __name__ == "__main__":
    main()
