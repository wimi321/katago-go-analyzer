#!/usr/bin/env python3
"""
æ™ºèƒ½æ–°é—»èšåˆå™¨ - å¤šæºæœç´¢ä¸è´¨é‡ç­›é€‰
"""

import json
import subprocess
from datetime import datetime, timedelta
from typing import List, Dict, Set
from collections import defaultdict

class NewsAggregator:
    def __init__(self):
        self.results = []
        self.seen_urls = set()
        
    def search_twitter(self, query: str, count: int = 10) -> List[Dict]:
        """ä½¿ç”¨birdæœç´¢Twitter"""
        try:
            cmd = ["bird", "search", query, "-n", str(count)]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode != 0:
                return []
            
            # è§£æbirdè¾“å‡º
            tweets = []
            lines = result.stdout.split('\n')
            current_tweet = {}
            
            for line in lines:
                if line.startswith('@'):
                    if current_tweet:
                        tweets.append(current_tweet)
                    # è§£æç”¨æˆ·å
                    parts = line.split('(')
                    if len(parts) >= 2:
                        current_tweet = {
                            'username': parts[0].strip(),
                            'display_name': parts[1].split(')')[0] if ')' in parts[1] else ''
                        }
                elif line.startswith('ğŸ“…'):
                    current_tweet['timestamp'] = line.replace('ğŸ“…', '').strip()
                elif line.startswith('ğŸ”—'):
                    current_tweet['url'] = line.replace('ğŸ”—', '').strip()
                elif line.strip() and not line.startswith('â”€'):
                    if 'text' not in current_tweet:
                        current_tweet['text'] = line.strip()
                    else:
                        current_tweet['text'] += ' ' + line.strip()
            
            if current_tweet:
                tweets.append(current_tweet)
                
            return tweets
            
        except Exception as e:
            print(f"Twitteræœç´¢å¤±è´¥: {e}")
            return []
    
    def calculate_relevance_score(self, tweet: Dict, keywords: List[str]) -> float:
        """è®¡ç®—ç›¸å…³æ€§å¾—åˆ†"""
        text = tweet.get('text', '').lower()
        score = 0.0
        
        # å…³é”®è¯åŒ¹é…
        for keyword in keywords:
            if keyword.lower() in text:
                score += 1.0
        
        # é•¿åº¦æƒ©ç½šï¼ˆè¿‡çŸ­å¯èƒ½æ˜¯spamï¼‰
        if len(text) < 50:
            score *= 0.5
        
        # URLå­˜åœ¨åŠ åˆ†
        if tweet.get('url'):
            score += 0.5
            
        return score
    
    def deduplicate(self, tweets: List[Dict]) -> List[Dict]:
        """å»é‡"""
        unique = []
        for tweet in tweets:
            url = tweet.get('url', '')
            if url and url not in self.seen_urls:
                self.seen_urls.add(url)
                unique.append(tweet)
        return unique
    
    def aggregate_by_category(self) -> Dict[str, List[Dict]]:
        """æŒ‰ç±»åˆ«èšåˆæœç´¢"""
        categories = {
            "AIçªç ´": [
                "DeepSeek R1", "Claude Opus", "GPT-5", "Gemini 3",
                "AI breakthrough", "AIæ¨¡å‹"
            ],
            "ç§‘æŠ€å…¬å¸": [
                "OpenAI", "Anthropic", "Google AI", "Meta AI",
                "Microsoft AI", "Apple Intelligence"
            ],
            "æ”¿ç­–ç»æµ": [
                "Trump tariff", "ç‰¹æœ—æ™®å…³ç¨", "ä¸­ç¾è´¸æ˜“",
                "ç»æµæ•°æ®", "policy change"
            ],
            "åŠ å¯†è´§å¸": [
                "Bitcoin", "Ethereum", "crypto", "Web3",
                "DeFi", "NFT"
            ]
        }
        
        results = defaultdict(list)
        
        for category, keywords in categories.items():
            print(f"\nğŸ” æœç´¢ç±»åˆ«: {category}")
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            query = " OR ".join([f'"{kw}"' for kw in keywords[:3]])  # é™åˆ¶æŸ¥è¯¢é•¿åº¦
            
            tweets = self.search_twitter(query, count=15)
            
            # è®¡ç®—ç›¸å…³æ€§å¹¶æ’åº
            scored_tweets = []
            for tweet in tweets:
                score = self.calculate_relevance_score(tweet, keywords)
                if score > 0:
                    tweet['relevance_score'] = score
                    scored_tweets.append(tweet)
            
            # æ’åºå¹¶å»é‡
            scored_tweets.sort(key=lambda x: x['relevance_score'], reverse=True)
            unique_tweets = self.deduplicate(scored_tweets)
            
            # å–top 5
            results[category] = unique_tweets[:5]
            print(f"  âœ“ æ‰¾åˆ° {len(unique_tweets)} æ¡ç›¸å…³æ¨æ–‡")
        
        return results
    
    def generate_briefing(self, categorized_results: Dict[str, List[Dict]]) -> str:
        """ç”Ÿæˆç®€æŠ¥"""
        now = datetime.now()
        briefing = f"# ğŸ“° æ¯æ—¥ç®€æŠ¥ {now.strftime('%Y-%m-%d')}\n\n"
        briefing += f"> ç”Ÿæˆæ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M')} GMT+8\n"
        briefing += f"> æ•°æ®æ¥æºï¼šX/Twitter æ™ºèƒ½èšåˆ\n"
        briefing += f"> æœç´¢ç­–ç•¥ï¼šåˆ†ç±»å…³é”®è¯ + ç›¸å…³æ€§æ’åº\n\n"
        briefing += "---\n\n"
        
        category_icons = {
            "AIçªç ´": "ğŸ¤–",
            "ç§‘æŠ€å…¬å¸": "ğŸ¢",
            "æ”¿ç­–ç»æµ": "ğŸ“Š",
            "åŠ å¯†è´§å¸": "ğŸ’°"
        }
        
        for category, tweets in categorized_results.items():
            if not tweets:
                continue
                
            icon = category_icons.get(category, "ğŸ“Œ")
            briefing += f"## {icon} {category}\n\n"
            
            for i, tweet in enumerate(tweets, 1):
                text = tweet.get('text', '')[:200]  # é™åˆ¶é•¿åº¦
                username = tweet.get('username', 'Unknown')
                url = tweet.get('url', '')
                timestamp = tweet.get('timestamp', '')
                score = tweet.get('relevance_score', 0)
                
                briefing += f"### {i}. @{username}\n"
                briefing += f"{text}\n\n"
                if url:
                    briefing += f"ğŸ”— [{url}]({url})\n"
                briefing += f"ğŸ“… {timestamp} | ç›¸å…³æ€§: {score:.1f}\n\n"
            
            briefing += "---\n\n"
        
        briefing += "## ğŸ’¡ æœç´¢è´¨é‡æŠ¥å‘Š\n\n"
        total_results = sum(len(tweets) for tweets in categorized_results.values())
        briefing += f"- æ€»ç»“æœæ•°: {total_results}\n"
        briefing += f"- å»é‡å: {len(self.seen_urls)}\n"
        briefing += f"- è¦†ç›–ç±»åˆ«: {len([c for c, t in categorized_results.items() if t])}\n\n"
        
        briefing += "*æœ¬ç®€æŠ¥ä½¿ç”¨æ™ºèƒ½èšåˆç®—æ³•ç”Ÿæˆï¼Œå·²ä¼˜åŒ–å…³é”®è¯ç­–ç•¥å’Œç›¸å…³æ€§æ’åº*\n"
        
        return briefing


def main():
    print("ğŸš€ å¯åŠ¨æ™ºèƒ½æ–°é—»èšåˆå™¨...")
    
    aggregator = NewsAggregator()
    
    # æŒ‰ç±»åˆ«èšåˆ
    results = aggregator.aggregate_by_category()
    
    # ç”Ÿæˆç®€æŠ¥
    briefing = aggregator.generate_briefing(results)
    
    # ä¿å­˜æ–‡ä»¶
    now = datetime.now()
    filename = f"/Users/haoc/.openclaw/workspace/briefing-{now.strftime('%Y-%m-%d')}-v2.md"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(briefing)
    
    print(f"\nâœ… ç®€æŠ¥å·²ç”Ÿæˆ: {filename}")
    print(f"ğŸ“Š æ€»è®¡ {sum(len(t) for t in results.values())} æ¡é«˜è´¨é‡ç»“æœ")


if __name__ == "__main__":
    main()
